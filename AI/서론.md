> No. I think we're moving into a period when for the first time ever we may have things more intelligent than us.
> - Geoffrey Hinton -


### 지능?
---

>인류는 아직 지능에 대한 합의된 정의가 없다.

관점이 너무 다양하고 많기 때문.

인공지능을 연구하는 사람들은 지능을 어떻게 볼것인가?

AGI 를 연구하는 연구가들의 입장.

>Our definition has been really consistent all the way through: this idea of having all the cognitive capabilities humans have.
>- Demis Hassabis - 

## 자연지능, 인공지능
---


Human Intelligence

Thinking -> Decision -> Action
l                                                    l
l---------- learning -----------l


## History Of Intelligence or MileStone of Intelligence
---

<img width="1536" height="1024" alt="Image" src="https://github.com/user-attachments/assets/03391c63-e96d-4f61-9be4-81fe5cc34655" />

1936년 투링의 계산가능성 이론에서 2023년 GPT-4까지, 인공지능(AI)은 **이론 → 하드웨어 → 기호추론 → 통계학습 → 딥러닝 → 거대 기반모델**이라는 다섯 차례의 물결을 거치며 발전.

#### 3.1 계산 이론·초기 하드웨어 (1936-1955)
---

-  AND·OR·NOT를 조합해 임의의 불 대수식을 계산할 수 있음을 증명 → **심볼릭 조작 능력**과 신경 메커니즘을 연결. [com-cog-book.github.io](https://com-cog-book.github.io/com-cog-book/features/mp-artificial-neuron.html?utm_source=chatgpt.com)
- 튜링이 증명한 “연산은 기호 재배열”과 호응해 **생물학적 지능도 계산으로 환원 가능**하다는 수학적 토대 제공

- **Z3 (1941)** : 릴레이 2 600개로 22-bit 부동 연산 수행, 천공필름에 명령을 저장 → **프로그램 가능한 기계**가 기호 조작을 전기-기계적으로 실현. [en.wikipedia.org](https://en.wikipedia.org/wiki/Z3_%28computer%29?utm_source=chatgpt.com)
- **Colossus (1944)** : 진공관 2 400개로 Lorenz 암호를 해독, 패치보드로 논리 함수를 변경 → **전자식 + 재배선**으로 Boolean 연산 가속. [en.wikipedia.org](https://en.wikipedia.org/wiki/Colossus_computer?utm_source=chatgpt.com)
- **시사점** :
    1. 추상 논리 → 물리적 회로 매핑이 가능함을 증명.
    2. 이후 폰 노이만 EDVAC 보고서가 **저장-프로그램 구조**를 정립하여 범용 컴퓨터 시대를 여는 촉매가 됨. [rodsmith.nz](https://rodsmith.nz/wp-content/uploads/Minsky-and-Papert-Perceptrons.pdf?utm_source=chatgpt.com)

계산 가능성 = 기호조작
물리적 컴퓨터로 구현.

#### 3.2 기호주의와 단층 신경망 (1956-1969)
---

다트머스 회의(1956)가 “AI라는 학문영역”을 선포하고  심볼릭 접근(추론·검색)을 중심 어젠다로 확립. [en.wikipedia.org](https://en.wikipedia.org/wiki/Dartmouth_workshop?utm_source=chatgpt.com)

- 다트머스 회의는 “언젠가 기계가 인간의 모든 학습 능력을 시뮬레이션할 것”이라 선언. [home.dartmouth.edu](https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth?utm_source=chatgpt.com)

동시기에 **Perceptron 하드웨어**가 ‘학습 가능한 기계’의 실증 데이터를 제공해, 연구 초점이 **인지 재현 vs. 학습 메커니즘**으로 양분됨.

| 용어                      | 한국어   | 설명                                                                                                                                                                |
| ----------------------- | ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Perceptron**          | 퍼셉트론  | TLU에 학습 규칙(가중치 갱신) 추가한 단층 신경망 [en.wikipedia.org](https://en.wikipedia.org/wiki/Perceptrons_%28book%29?utm_source=chatgpt.com)                                     |
| **Linear separability** | 선형 분리 | 데이터 집합을 **단일 초평면**으로 완벽히 양분할 수 있는 성질                                                                                                                              |
| **한계**                  | —     | XOR 등 비선형 패턴은 단층이 분리 불가 → Minsky-Papert가 1969년 증명, AI 회의 열기 급속 냉각 [en.wikipedia.org](https://en.wikipedia.org/wiki/Perceptrons_%28book%29?utm_source=chatgpt.com) |
| **다층 학습 연구**            | —     | 오류 역전파(back-prop)·사전학습 등 ‘딥’ 구조 탐색 모티브 제공                                                                                                                         |

- 퍼셉트론의 성공은 곧 한계(선형 분리 한정) 노출로 이어졌고, 이는 이후 다층 학습 연구의 동기를 제공. [ling.upenn.edu](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf?utm_source=chatgpt.com)

#### 3.3 붐-버스트와 통계적 ML (1970-1999)
---

- 1973 Lighthill·1969 기계번역 실패로 **첫 겨울**; 1987 Lisp-머신 붕괴로 **두번째 겨울** 도래.

- 통계학·패턴인식계가 확률·커널 기반 ‘머신러닝’ 을 독자 진화.

- **XCON** (Expert-Configurator) : DEC VAX 서버 부품 5 000여개 규칙을 체계화해 주문 오류 31%→0.5%로 축소, 연 2천만 달러 절감으로 **AI 실용 부활**을 이끎

통계적 학습의 대안

|개념|설명|
|---|---|
|**Statistical learning (통계적 학습)**|경험 데이터 → 확률 모델 추정; 소음·불확실성을 수학적 모형에 내재화해 **일반화**를 강조|
|**SVM**|고차원 내에서 **최대-마진** 초평면을 찾는 커널 기반 분류기, 작은 샘플에서도 오버피팅 억제 [link.springer.com](https://link.springer.com/article/10.1007/BF00994018?utm_source=chatgpt.com)|
|**High-dimensional (고차원)**|특징 공간 d≫nd \gg nd≫n 시나리오; 커널 트릭으로 **무한 차원**까지 확대|
|**Generalization (일반화)**|학습 외 데이터로도 낮은 오류를 유지하는 성질; SVM 이론은 VC 차원·마진을 통해 이를 정량화|

 Deep Blue — Search + Heuristic

- **검색(search)** : 200 million 노드/s Alpha-Beta 탐색.
    
- **휴리스틱(heuristic) 평가** : 체스 말 가치·위치 점수를 손수 설계해 **가지치기 효율**을 극대화. [en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_Blue_%28chess_computer%29?utm_source=chatgpt.com)
    
- **위력** : 1997년 세계 챔프 카스파로프 격파 → 대중이 AI 잠재력을 재인식.

#### 3.4 딥러닝 르네상스 (2000-2016)
---

| 주제                                    | 해설                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Layer-wise pre-training (층별 사전학습)** | RBM or AE를 한 층씩 학습해 초기 가중치를 좋은 영역에 배치, 이후 미세조정; **기울기 소실** 완화 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_belief_network?utm_source=chatgpt.com "Deep belief network - Wikipedia"), [machinelearningmastery.com](https://www.machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/?utm_source=chatgpt.com "How to Use Greedy Layer-Wise Pretraining in Deep Learning ..."))                                          |
| Vanishing gradient (기울기 소실)           | 역전파 시 깊은 층의 ∂E/∂W가 0에 수렴해 학습 정체되는 현상 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Vanishing_gradient_problem?utm_source=chatgpt.com "Vanishing gradient problem - Wikipedia"))                                                                                                                                                                                                                                                          |
| **GPU acceleration**                  | 대규모 행렬곱(Conv·GEMM)을 CUDA로 병렬 실행, **GTX580 2장**이 CPU 2 000개 성능 ([blogs.nvidia.com](https://blogs.nvidia.com/blog/accelerating-ai-artificial-intelligence-gpus/?utm_source=chatgpt.com "Accelerating AI with GPUs: A New Computing Model - NVIDIA Blog"), [en.wikipedia.org](https://en.wikipedia.org/wiki/AlexNet?utm_source=chatgpt.com "AlexNet")) → 이미지·음성 빅데이터 학습 ‘실시간’화                                                                  |
| **Google Brain ‘Cat’ 실험**             | 16 000 CPU(1 B 매개) 비지도 학습으로 _cat concept_ 출현 → **컴퓨트 확대만으로 고수준(high-level) 특징** 자동 학습 가능함을 입증 ([wired.com](https://www.wired.com/2012/06/google-x-neural-network/?utm_source=chatgpt.com "Google's Artificial Brain Learns to Find Cat Videos - WIRED"), [wired.com](https://www.wired.com/2012/06/google-x-neural-network?utm_source=chatgpt.com "Google's Artificial Brain Learns to Find Cat Videos"))                                    |
| **AlphaGo 정책-가치망 + 탐색**               | ① Policy Network(정책망) : 다음 수 분포, ② Value Network(가치망) : 승률 예측, ③ MCTS 탐색과 결합 → ‘**직관 (신경망)** + 계산 (트리)’ 하이브리드 패러다임 ([nature.com](https://www.nature.com/articles/nature16961?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search"), [pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/26819042/?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search")) |

#### 3.5 기반모델·계획 강화 단계 (2017-현재)

| 용어 (영)                         | 한국어         | 설명                                                                                                                                                                                                                                                                                                                                 |
| ------------------------------ | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Transformer**                | 트랜스포머       | Self-attention으로 **장기 의존(long-range dependency)**를 O(1)-경로로 캡처, GPU/TPU 대규모 병렬화에 최적 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_belief_network?utm_source=chatgpt.com "Deep belief network - Wikipedia"))                                                                                                               |
| **Vision·Speech & Multimodal** | 비전·음성·멀티모달  | 동일 아키텍처를 **이미지·음향·텍스트**로 확장해 단일 모델이 복합 자극을 처리                                                                                                                                                                                                                                                                                      |
| **MuZero**                     | 뮤제로         | 보드·아타리에서 **라틴트(latent) 동역학** f_θ 를 자가 학습, 규칙 모름에도 계획 성공 → **범용 계획 AI** 시사 ([nature.com](https://www.nature.com/articles/nature16961?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search"))                                                                                                  |
| **RLHF**                       | 인간-피드백 강화학습 | 모델 출력에 인간 선호 점수를 부여해 정책을 fine-tune ; ChatGPT 안전·품질 핵심 기술 ([openai.com](https://openai.com/index/chatgpt/?utm_source=chatgpt.com "Introducing ChatGPT - OpenAI"), [wired.com](https://www.wired.com/story/openai-rlhf-ai-training?utm_source=chatgpt.com "OpenAI Wants AI to Help Humans Train AI"))                                |
| **Instruction tuning**         | 인스트럭션 튜닝    | “[Prompt, Ideal-Answer]” 쌍으로 지도 미세조정해 **지시 수행 능력**을 범용 향상 ([arxiv.org](https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com "Training language models to follow instructions with human feedback"), [ibm.com](https://www.ibm.com/think/topics/instruction-tuning?utm_source=chatgpt.com "What Is Instruction Tuning? \| IBM")) |
| **Foundation Model**           | 기반 모델       | 초거대 사전학습 → **Downstream few-shot**으로 범용 전이; GPT-4는 멀티모달·API 생태계로 확장 ([openai.com](https://openai.com/index/gpt-4-research/?utm_source=chatgpt.com "GPT-4 - OpenAI"), [en.wikipedia.org](https://en.wikipedia.org/wiki/GPT-4?utm_source=chatgpt.com "GPT-4"))                                                                       |

## 추론과 유추
---

지식/경험/사실/법칙 -> 가설/유추

Logic - Rule Based = Symbolic Ai

