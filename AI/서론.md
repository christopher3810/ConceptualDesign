> No. I think we're moving into a period when for the first time ever we may have things more intelligent than us.
> - Geoffrey Hinton -


### 지능?
---

>인류는 아직 지능에 대한 합의된 정의가 없다.

관점이 너무 다양하고 많기 때문.

인공지능을 연구하는 사람들은 지능을 어떻게 볼것인가?

AGI 를 연구하는 연구가들의 입장.

>Our definition has been really consistent all the way through: this idea of having all the cognitive capabilities humans have.
>- Demis Hassabis - 

## 자연지능, 인공지능
---


Human Intelligence

```text
Thinking -> Decision -> Action
l                             l
l---------- learning ---------l
```


## History Of Intelligence or MileStone of Intelligence
---

<img width="1536" height="1024" alt="Image" src="https://github.com/user-attachments/assets/03391c63-e96d-4f61-9be4-81fe5cc34655" />

1936년 투링의 계산가능성 이론에서 2023년 GPT-4까지, 인공지능(AI)은 **이론 → 하드웨어 → 기호추론 → 통계학습 → 딥러닝 → 거대 기반모델**이라는 다섯 차례의 물결을 거치며 발전.

#### 3.1 계산 이론·초기 하드웨어 (1936-1955)
---

-  AND·OR·NOT를 조합해 임의의 불 대수식을 계산할 수 있음을 증명 → **심볼릭 조작 능력**과 신경 메커니즘을 연결. [com-cog-book.github.io](https://com-cog-book.github.io/com-cog-book/features/mp-artificial-neuron.html?utm_source=chatgpt.com)
- 튜링이 증명한 “연산은 기호 재배열”과 호응해 **생물학적 지능도 계산으로 환원 가능**하다는 수학적 토대 제공

- **Z3 (1941)** : 릴레이 2 600개로 22-bit 부동 연산 수행, 천공필름에 명령을 저장 → **프로그램 가능한 기계**가 기호 조작을 전기-기계적으로 실현. [en.wikipedia.org](https://en.wikipedia.org/wiki/Z3_%28computer%29?utm_source=chatgpt.com)
- **Colossus (1944)** : 진공관 2 400개로 Lorenz 암호를 해독, 패치보드로 논리 함수를 변경 → **전자식 + 재배선**으로 Boolean 연산 가속. [en.wikipedia.org](https://en.wikipedia.org/wiki/Colossus_computer?utm_source=chatgpt.com)
- **시사점** :
    1. 추상 논리 → 물리적 회로 매핑이 가능함을 증명.
    2. 이후 폰 노이만 EDVAC 보고서가 **저장-프로그램 구조**를 정립하여 범용 컴퓨터 시대를 여는 촉매가 됨. [rodsmith.nz](https://rodsmith.nz/wp-content/uploads/Minsky-and-Papert-Perceptrons.pdf?utm_source=chatgpt.com)

계산 가능성 = 기호조작
물리적 컴퓨터로 구현.

#### 3.2 기호주의와 단층 신경망 (1956-1969)
---

다트머스 회의(1956)가 “AI라는 학문영역”을 선포하고  심볼릭 접근(추론·검색)을 중심 어젠다로 확립. [en.wikipedia.org](https://en.wikipedia.org/wiki/Dartmouth_workshop?utm_source=chatgpt.com)

- 다트머스 회의는 “언젠가 기계가 인간의 모든 학습 능력을 시뮬레이션할 것”이라 선언. [home.dartmouth.edu](https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth?utm_source=chatgpt.com)

동시기에 **Perceptron 하드웨어**가 ‘학습 가능한 기계’의 실증 데이터를 제공해, 연구 초점이 **인지 재현 vs. 학습 메커니즘**으로 양분됨.

| 용어                      | 한국어   | 설명                                                                                                                                                                |
| ----------------------- | ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Perceptron**          | 퍼셉트론  | TLU에 학습 규칙(가중치 갱신) 추가한 단층 신경망 [en.wikipedia.org](https://en.wikipedia.org/wiki/Perceptrons_%28book%29?utm_source=chatgpt.com)                                     |
| **Linear separability** | 선형 분리 | 데이터 집합을 **단일 초평면**으로 완벽히 양분할 수 있는 성질                                                                                                                              |
| **한계**                  | —     | XOR 등 비선형 패턴은 단층이 분리 불가 → Minsky-Papert가 1969년 증명, AI 회의 열기 급속 냉각 [en.wikipedia.org](https://en.wikipedia.org/wiki/Perceptrons_%28book%29?utm_source=chatgpt.com) |
| **다층 학습 연구**            | —     | 오류 역전파(back-prop)·사전학습 등 ‘딥’ 구조 탐색 모티브 제공                                                                                                                         |

- 퍼셉트론의 성공은 곧 한계(선형 분리 한정) 노출로 이어졌고, 이는 이후 다층 학습 연구의 동기를 제공. [ling.upenn.edu](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf?utm_source=chatgpt.com)

#### 3.3 붐-버스트와 통계적 ML (1970-1999)
---

- 1973 Lighthill·1969 기계번역 실패로 **첫 겨울**; 1987 Lisp-머신 붕괴로 **두번째 겨울** 도래.

- 통계학·패턴인식계가 확률·커널 기반 ‘머신러닝’ 을 독자 진화.

- **XCON** (Expert-Configurator) : DEC VAX 서버 부품 5 000여개 규칙을 체계화해 주문 오류 31%→0.5%로 축소, 연 2천만 달러 절감으로 **AI 실용 부활**을 이끎

통계적 학습의 대안

|개념|설명|
|---|---|
|**Statistical learning (통계적 학습)**|경험 데이터 → 확률 모델 추정; 소음·불확실성을 수학적 모형에 내재화해 **일반화**를 강조|
|**SVM**|고차원 내에서 **최대-마진** 초평면을 찾는 커널 기반 분류기, 작은 샘플에서도 오버피팅 억제 [link.springer.com](https://link.springer.com/article/10.1007/BF00994018?utm_source=chatgpt.com)|
|**High-dimensional (고차원)**|특징 공간 d≫nd \gg nd≫n 시나리오; 커널 트릭으로 **무한 차원**까지 확대|
|**Generalization (일반화)**|학습 외 데이터로도 낮은 오류를 유지하는 성질; SVM 이론은 VC 차원·마진을 통해 이를 정량화|

 Deep Blue — Search + Heuristic

- **검색(search)** : 200 million 노드/s Alpha-Beta 탐색.
    
- **휴리스틱(heuristic) 평가** : 체스 말 가치·위치 점수를 손수 설계해 **가지치기 효율**을 극대화. [en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_Blue_%28chess_computer%29?utm_source=chatgpt.com)
    
- **위력** : 1997년 세계 챔프 카스파로프 격파 → 대중이 AI 잠재력을 재인식.

#### 3.4 딥러닝 르네상스 (2000-2016)
---

| 주제                                    | 해설                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Layer-wise pre-training (층별 사전학습)** | RBM or AE를 한 층씩 학습해 초기 가중치를 좋은 영역에 배치, 이후 미세조정; **기울기 소실** 완화 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_belief_network?utm_source=chatgpt.com "Deep belief network - Wikipedia"), [machinelearningmastery.com](https://www.machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/?utm_source=chatgpt.com "How to Use Greedy Layer-Wise Pretraining in Deep Learning ..."))                                          |
| Vanishing gradient (기울기 소실)           | 역전파 시 깊은 층의 ∂E/∂W가 0에 수렴해 학습 정체되는 현상 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Vanishing_gradient_problem?utm_source=chatgpt.com "Vanishing gradient problem - Wikipedia"))                                                                                                                                                                                                                                                          |
| **GPU acceleration**                  | 대규모 행렬곱(Conv·GEMM)을 CUDA로 병렬 실행, **GTX580 2장**이 CPU 2 000개 성능 ([blogs.nvidia.com](https://blogs.nvidia.com/blog/accelerating-ai-artificial-intelligence-gpus/?utm_source=chatgpt.com "Accelerating AI with GPUs: A New Computing Model - NVIDIA Blog"), [en.wikipedia.org](https://en.wikipedia.org/wiki/AlexNet?utm_source=chatgpt.com "AlexNet")) → 이미지·음성 빅데이터 학습 ‘실시간’화                                                                  |
| **Google Brain ‘Cat’ 실험**             | 16 000 CPU(1 B 매개) 비지도 학습으로 _cat concept_ 출현 → **컴퓨트 확대만으로 고수준(high-level) 특징** 자동 학습 가능함을 입증 ([wired.com](https://www.wired.com/2012/06/google-x-neural-network/?utm_source=chatgpt.com "Google's Artificial Brain Learns to Find Cat Videos - WIRED"), [wired.com](https://www.wired.com/2012/06/google-x-neural-network?utm_source=chatgpt.com "Google's Artificial Brain Learns to Find Cat Videos"))                                    |
| **AlphaGo 정책-가치망 + 탐색**               | ① Policy Network(정책망) : 다음 수 분포, ② Value Network(가치망) : 승률 예측, ③ MCTS 탐색과 결합 → ‘**직관 (신경망)** + 계산 (트리)’ 하이브리드 패러다임 ([nature.com](https://www.nature.com/articles/nature16961?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search"), [pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/26819042/?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search")) |

#### 3.5 기반모델·계획 강화 단계 (2017-현재)

| 용어 (영)                         | 한국어         | 설명                                                                                                                                                                                                                                                                                                                                 |
| ------------------------------ | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Transformer**                | 트랜스포머       | Self-attention으로 **장기 의존(long-range dependency)**를 O(1)-경로로 캡처, GPU/TPU 대규모 병렬화에 최적 ([en.wikipedia.org](https://en.wikipedia.org/wiki/Deep_belief_network?utm_source=chatgpt.com "Deep belief network - Wikipedia"))                                                                                                               |
| **Vision·Speech & Multimodal** | 비전·음성·멀티모달  | 동일 아키텍처를 **이미지·음향·텍스트**로 확장해 단일 모델이 복합 자극을 처리                                                                                                                                                                                                                                                                                      |
| **MuZero**                     | 뮤제로         | 보드·아타리에서 **라틴트(latent) 동역학** f_θ 를 자가 학습, 규칙 모름에도 계획 성공 → **범용 계획 AI** 시사 ([nature.com](https://www.nature.com/articles/nature16961?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and tree search"))                                                                                                  |
| **RLHF**                       | 인간-피드백 강화학습 | 모델 출력에 인간 선호 점수를 부여해 정책을 fine-tune ; ChatGPT 안전·품질 핵심 기술 ([openai.com](https://openai.com/index/chatgpt/?utm_source=chatgpt.com "Introducing ChatGPT - OpenAI"), [wired.com](https://www.wired.com/story/openai-rlhf-ai-training?utm_source=chatgpt.com "OpenAI Wants AI to Help Humans Train AI"))                                |
| **Instruction tuning**         | 인스트럭션 튜닝    | “[Prompt, Ideal-Answer]” 쌍으로 지도 미세조정해 **지시 수행 능력**을 범용 향상 ([arxiv.org](https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com "Training language models to follow instructions with human feedback"), [ibm.com](https://www.ibm.com/think/topics/instruction-tuning?utm_source=chatgpt.com "What Is Instruction Tuning? \| IBM")) |
| **Foundation Model**           | 기반 모델       | 초거대 사전학습 → **Downstream few-shot**으로 범용 전이; GPT-4는 멀티모달·API 생태계로 확장 ([openai.com](https://openai.com/index/gpt-4-research/?utm_source=chatgpt.com "GPT-4 - OpenAI"), [en.wikipedia.org](https://en.wikipedia.org/wiki/GPT-4?utm_source=chatgpt.com "GPT-4"))                                                                       |

## 추론과 유추
---

지식/경험/사실/법칙 -> 가설/유추

사람의 지식, 경험, Rule Base 로 만들어왔음.
Symbolic AI

AI를 나눌때 Symbolic AI, Machine Learning 을 구분함
Machine Learning은 학습을 하는것.

Symbolic AI는 규칙을 만듬.

논리력도 사실은 학습을 통해서 얻을수 있지 않을까?
학습을 통해서 논리력을 만드는 모델을 개발하는데 개발중.

### 자연어 처리
---

>사람이 쓰는 언어를 컴퓨터가 사용하는 언어로 바꾸는 것.

① **텍스트 정규화·토큰화**로 _문자를 이산 심볼_로 만들고,  
② **벡터화/임베딩**으로 _실수 공간_에 매핑해 연산 효율을 확보한 뒤,  
③ **연관성(relevance) 분석**으로 토큰·문장 간 관계를 수량화하며,  
④ **어텐션 기반 Transformer**로 문맥적 의존을 학습하고,  
⑤ **자기회귀(autoregressive) 언어모델**로 다음 토큰을 순차 예측해 텍스트를 생성합니다.  

>이 순서는 “이산 → 연속 → 관계 → 추론 → 생성”이라는 논리적 계단으로 이어짐.

#### 텍스트 정규화와 토큰화

왜 필요한가?

컴퓨터는 바이트 단위로만 연산하므로 “**단어·형태소·부분단어**” 같은 의미 단위를 **정수 ID**로 치환해야 통계·신경망이 입력을 다룰 수 있음.[medium.com](https://medium.com/%40sateeshfrnd/tokenization-and-normalization-in-nlp-30f7c3c4a26b?utm_source=chatgpt.com)

#### 토큰화 방법

|방식|장점|한계|
|---|---|---|
|**Word-level** (단어)|구현 단순|희귀어(OOV) 폭증, 한국어·독일어 등 교착·복합어 처리가 불리|
|**Character-level** (문자)|OOV 없음|시퀀스 길이 ↑, 의미 단위 손실|
|**Sub-word** (BPE, Unigram, **SentencePiece**)|OOV↓·길이↓ 균형|사전 학습 필요, 분할 결과가 모델 품질 의존 [medium.com](https://medium.com/codex/sentencepiece-a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-ffda431e704e?utm_source=chatgpt.com)[github.com](https://github.com/google/sentencepiece?utm_source=chatgpt.com)|

> 예시: “기계학습” → `기계@@ 학@@ 습` (BPE) → ID `\[421, 1562, 98]`

#### 벡터화(Vectorization)·임베딩(Embedding)

#### One-hot → 밀집(dense) 임베딩

초창기에는 토큰 ID를 **One-hot** 벡터로 사용했으나 차원이 방대하고 의미 유사도를 반영하지 못합니다. **Word2Vec(CBOW·Skip-gram)** 은 주변 단어 예측을 통해 수십 차원 밀집 벡터를 학습, “왕-남 + 여 ≈ 여왕”과 같은 산술적 의미 조합을 가능케 했습니다.[medium.com](https://medium.com/data-science/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314?utm_source=chatgpt.com)[baeldung.com](https://www.baeldung.com/cs/word-embeddings-cbow-vs-skip-gram?utm_source=chatgpt.com)

#### Static → Contextual

단어 의미는 문맥에 따라 변합니다. **ELMo·BERT·GPT** 는 **문맥적(contextual) 임베딩**을 도입, 동일 단어라도 위치별 벡터를 달리해 다의성을 해결했습니다.[ai.stanford.edu](https://ai.stanford.edu/blog/contextual/?utm_source=chatgpt.com)[paperswithcode.com](https://paperswithcode.com/method/elmo?utm_source=chatgpt.com)
#### Position Embedding

Transformer는 순서를 직접 다루지 않으므로, 사인·코사인 함수나 학습 가능 벡터로 **Positional Encoding**을 추가해 “토큰 i의 위치정보”를 전달합니다.

#### 연관성(관련성) 분석 단계

#### 통계적 접근

- **TF-IDF / BM25** : 문서 내 빈도와 길이를 보정해 단어·문서 간 유사도를 산출, 정보검색·RAG 시스템의 1차 스코어러로 널리 쓰입니다.[analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/?utm_source=chatgpt.com)[medium.com](https://medium.com/%40tspann/ranking-for-relevance-with-bm25-b2d9dd62e2f8?utm_source=chatgpt.com)
    
- **PMI·KL Divergence** : 단어 공존 확률이 기대치보다 높을 때 연관성을 수치화.

#### 신경망적 접근 – Self-Attention

Transformer의 **Self-Attention**은 **Query·Key·Value(Q/K/V)** 삼중 연산으로 모든 토큰 쌍의 유사도를 실시간 계산해 _soft alignment_를 얻게 함.[stats.stackexchange.com](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms?utm_source=chatgpt.com)[sebastianraschka.com](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html?utm_source=chatgpt.com) 연관성이 높을수록 가중치가 커져 정보가 더 많이 전달됨.

#### Transformer & Attention (트랜스포머와 어텐션)

| 개념(영)                         | 한국어         | 역할                                      |
| ----------------------------- | ----------- | --------------------------------------- |
| **Multi-Head Self-Attention** | 다중 헤드 자기어텐션 | 서로 다른 표현 공간에서 Q/K/V를 병렬 계산해 복수 관계 패턴 포착 |
| **Feed-Forward Network**      | 전결합 계층      | 어텐션 출력에 비선형 변환(GeLU·ReLU) 적용            |
| **Residual & LayerNorm**      | 잔차·정규화      | 안정적 역전파와 학습 속도 개선                       |

어텐션이 “관련성(weight)×값(Value)”의 합으로 출력을 만들기 때문에, **문맥적 중요도**를 학습 중에 자동 조정할 수 있다.[medium.com](https://medium.com/%40lmpo/understanding-self-attention-and-transformer-network-architecture-0734f73b8fa3?utm_source=chatgpt.com)
#### 자기회귀 (Autoregressive) 생성

#### 학습 목표

**Language Modeling Objective**: 조건 P(wt ∣ w<t)P(w_t | w_{<t})P(wt​ ∣ w<t​) 를 최대화해 ‘다음 토큰’을 예측. 
GPT 류는 완전 **자기회귀(decoder-only)** 구조로 훈련.[huggingface.co](https://huggingface.co/blog/alonsosilva/nexttokenprediction?utm_source=chatgpt.com)[arxiv.org](https://arxiv.org/abs/2411.15661?utm_source=chatgpt.com)

#### 추론(디코딩)

1. **Greedy / Beam Search** – 확률 최대 토큰을 직행 혹은 상위 k 경로 탐색.

2. **Sampling** – Top-k, Top-p(Nucleus)로 창의성·다양성 제어.

3. **Temperature** – 분포 평탄화로 무작위성 조절.

#### 장점·제한

- 장점: 텍스트 순차 생성·코드 자동완성 등 **실시간 인터랙션** 가능.
    
- 제약: 긴 의존 관계에서 _exposure bias_, 계산량 O(T2)O(T^2)O(T2) 등 과제가 남아, **Mixture-of-Experts·Flash-Attention** 같은 최적화를 연구 중

### 현재 인공지능 트렌드
---

신경망 기반 딥러닝 모델

트랜스포머 형태의 모델 구조

학습모델의 거대화

생성모델의 확산

강화학습과 메타학습의 성과